{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMP529 Assignment2 Part 1. Data Analysis using PySpark**\n",
    "\n",
    "Lizhenghe.Chen 201521681\n",
    "\n",
    "I will use Notebook as my part1 report\n",
    "\n",
    "Can run my code successfully in VM with swiched java version 8 (1.8) and python 2.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RA5I8KevCP-l",
    "outputId": "2896be8c-13c8-4c4c-c6b6-284faf465e08"
   },
   "outputs": [],
   "source": [
    "# import necessary package and intialize spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Can think of it as multithreading\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"COMP529\").getOrCreate()\n",
    "path = \"dataset.txt\"  # the file path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read txt data using .csv format from spark, also specificate format and data type\n",
    "you can complre the read file speed between Spark and Pandas, since the dataset is still small: 2537331x8, so the difference isn't obvious, but Spark is definitely faster for big data:Pandas read and print: 2.6s; Spark read and print: 0.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15547\\AppData\\Local\\Temp\\ipykernel_24044\\3908080286.py:3: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(path, header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UserID</td>\n",
       "      <td>Latitude</td>\n",
       "      <td>Longitude</td>\n",
       "      <td>AllZero</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>Timestamp</td>\n",
       "      <td>Date</td>\n",
       "      <td>Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>39.974408918</td>\n",
       "      <td>116.303522101</td>\n",
       "      <td>0</td>\n",
       "      <td>480.287355643045</td>\n",
       "      <td>40753.5306944444</td>\n",
       "      <td>2011-07-29</td>\n",
       "      <td>12:44:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>39.974397078</td>\n",
       "      <td>116.303526932</td>\n",
       "      <td>0</td>\n",
       "      <td>480.121151574803</td>\n",
       "      <td>40753.5307060185</td>\n",
       "      <td>2011-07-29</td>\n",
       "      <td>12:44:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>39.973982524</td>\n",
       "      <td>116.303621837</td>\n",
       "      <td>0</td>\n",
       "      <td>478.499455380577</td>\n",
       "      <td>40753.5307291667</td>\n",
       "      <td>2011-07-29</td>\n",
       "      <td>12:44:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>39.973943291</td>\n",
       "      <td>116.303632641</td>\n",
       "      <td>0</td>\n",
       "      <td>479.176988188976</td>\n",
       "      <td>40753.5307407407</td>\n",
       "      <td>2011-07-29</td>\n",
       "      <td>12:44:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537326</th>\n",
       "      <td>130</td>\n",
       "      <td>39.9753</td>\n",
       "      <td>116.330264</td>\n",
       "      <td>0</td>\n",
       "      <td>146.78855</td>\n",
       "      <td>40098.545093</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>13:04:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537327</th>\n",
       "      <td>130</td>\n",
       "      <td>39.975259</td>\n",
       "      <td>116.330317</td>\n",
       "      <td>0</td>\n",
       "      <td>142.033094</td>\n",
       "      <td>40098.545127</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>13:04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537328</th>\n",
       "      <td>130</td>\n",
       "      <td>39.975224</td>\n",
       "      <td>116.330385</td>\n",
       "      <td>0</td>\n",
       "      <td>139.996073</td>\n",
       "      <td>40098.545162</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>13:05:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537329</th>\n",
       "      <td>130</td>\n",
       "      <td>39.975193</td>\n",
       "      <td>116.330448</td>\n",
       "      <td>0</td>\n",
       "      <td>139.141322</td>\n",
       "      <td>40098.545185</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>13:05:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537330</th>\n",
       "      <td>130</td>\n",
       "      <td>39.97515</td>\n",
       "      <td>116.330476</td>\n",
       "      <td>0</td>\n",
       "      <td>138.587864</td>\n",
       "      <td>40098.545231</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>13:05:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2537331 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1              2        3                 4  \\\n",
       "0        UserID      Latitude      Longitude  AllZero          Altitude   \n",
       "1           100  39.974408918  116.303522101        0  480.287355643045   \n",
       "2           100  39.974397078  116.303526932        0  480.121151574803   \n",
       "3           100  39.973982524  116.303621837        0  478.499455380577   \n",
       "4           100  39.973943291  116.303632641        0  479.176988188976   \n",
       "...         ...           ...            ...      ...               ...   \n",
       "2537326     130       39.9753     116.330264        0         146.78855   \n",
       "2537327     130     39.975259     116.330317        0        142.033094   \n",
       "2537328     130     39.975224     116.330385        0        139.996073   \n",
       "2537329     130     39.975193     116.330448        0        139.141322   \n",
       "2537330     130      39.97515     116.330476        0        138.587864   \n",
       "\n",
       "                        5           6         7  \n",
       "0               Timestamp        Date      Time  \n",
       "1        40753.5306944444  2011-07-29  12:44:12  \n",
       "2        40753.5307060185  2011-07-29  12:44:13  \n",
       "3        40753.5307291667  2011-07-29  12:44:15  \n",
       "4        40753.5307407407  2011-07-29  12:44:16  \n",
       "...                   ...         ...       ...  \n",
       "2537326      40098.545093  2009-10-12  13:04:56  \n",
       "2537327      40098.545127  2009-10-12  13:04:59  \n",
       "2537328      40098.545162  2009-10-12  13:05:02  \n",
       "2537329      40098.545185  2009-10-12  13:05:04  \n",
       "2537330      40098.545231  2009-10-12  13:05:08  \n",
       "\n",
       "[2537331 rows x 8 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(path, header=None)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaMPH6bsFMkz",
    "outputId": "7e057bb3-bf47-4cdc-b115-71711766e8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- AllZero: string (nullable = true)\n",
      " |-- Altitude: double (nullable = true)\n",
      " |-- Timestamp: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      "\n",
      "+------+---------+----------+-------+----------------+----------------+----------+--------+\n",
      "|UserID| Latitude| Longitude|AllZero|        Altitude|       Timestamp|      Date|    Time|\n",
      "+------+---------+----------+-------+----------------+----------------+----------+--------+\n",
      "|   130|39.975088| 116.33269|      0|492.126135170604|40000.7996759259|2009-07-06|19:11:32|\n",
      "|   130|39.974945|116.334041|      0|479.549799868766|40000.8007638889|2009-07-06|19:13:06|\n",
      "|   130| 39.97504|116.332806|      0|491.743412073491|   40000.7996875|2009-07-06|19:11:33|\n",
      "|   130|39.975009|116.332997|      0|491.676630577428|40000.7996990741|2009-07-06|19:11:34|\n",
      "|   130|39.975048|116.332932|      0|491.403044619423|40000.7997106481|2009-07-06|19:11:35|\n",
      "|   130|39.974977| 116.33305|      0|491.043900918635|40000.7997337963|2009-07-06|19:11:37|\n",
      "|   130|39.974967|116.333116|      0|489.435272309711| 40000.799849537|2009-07-06|19:11:47|\n",
      "|   130|39.974931|116.333188|      0|487.797303149606|40000.7999652778|2009-07-06|19:11:57|\n",
      "|   130|39.974927|116.333249|      0|489.250744750656|40000.8000347222|2009-07-06|19:12:03|\n",
      "|   130|39.974953| 116.33331|      0|487.910994094488|40000.8001388889|2009-07-06|19:12:12|\n",
      "|   130|39.974988|116.333359|      0|486.979045275591|40000.8001967593|2009-07-06|19:12:17|\n",
      "|   130|39.975029|116.333311|      0|486.138612204724|40000.8002777778|2009-07-06|19:12:24|\n",
      "|   130|39.974993|116.333368|      0|485.249717847769|40000.8003240741|2009-07-06|19:12:28|\n",
      "|   130|39.975013|116.333424|      0|484.258648293963|40000.8004050926|2009-07-06|19:12:35|\n",
      "|   130|39.975008|  116.3335|      0|483.536105643045| 40000.800462963|2009-07-06|19:12:40|\n",
      "|   130|39.975018|116.333564|      0|482.802204724409|40000.8005208333|2009-07-06|19:12:45|\n",
      "|   130|39.974994|116.333631|      0|482.616423884514|40000.8005324074|2009-07-06|19:12:46|\n",
      "|   130|   39.975|116.333703|      0|482.115708661417|40000.8005555556|2009-07-06|19:12:48|\n",
      "|   130|39.975025|116.333767|      0|481.127742782152|40000.8006365741|2009-07-06|19:12:55|\n",
      "|   130|39.974989|116.333882|      0|480.362650918635|40000.8006944444|2009-07-06|19:13:00|\n",
      "+------+---------+----------+-------+----------------+----------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").csv(path)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"UserID\", df.UserID.cast(IntegerType()))\n",
    "    .withColumn(\"Latitude\", df.Latitude.cast(DoubleType()))\n",
    "    .withColumn(\"Longitude\", df.Longitude.cast(DoubleType()))\n",
    "    .withColumn(\"Altitude\", df.Altitude.cast(DoubleType()))\n",
    "    .withColumn(\"Timestamp\", df.Timestamp.cast(DoubleType()))\n",
    "    .withColumn(\"Date\", df.Date.cast(DateType()))\n",
    ")\n",
    "df.printSchema()\n",
    "df.orderBy(col(\"UserID\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsmJM3Y-5Lu0"
   },
   "source": [
    "# **1.**\n",
    "\n",
    "To make all dates and times add 8 hours to Beijing time, my idea is, first combine \"Date\" and \"Time\" to DateTime, which include date and time at the same time;\n",
    "\n",
    "Then convert this new column to timestamp datatype that the computer can recognize date and time. The final step is straightforward: minus 8 hours for \"DateTime\", the computer will judge and calculate whether the date also needs to change if the time is earlier than 8:00 am.\n",
    "\n",
    "For the \"TimeStamp\", as we already know, it is the number of days that have passed since 12/30/1899. So to minuses 8 hours, it means minus 8/24 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Q3YcG4d6KI1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+-------+----------------+----------------+----------+--------+-------------------+\n",
      "|UserID|    Latitude|    Longitude|AllZero|        Altitude|       Timestamp|      Date|    Time|           DateTime|\n",
      "+------+------------+-------------+-------+----------------+----------------+----------+--------+-------------------+\n",
      "|   100|39.974408918|116.303522101|      0|480.287355643045|40753.5306944444|2011-07-29|12:44:12|2011-07-29 12:44:12|\n",
      "|   100|39.974397078|116.303526932|      0|480.121151574803|40753.5307060185|2011-07-29|12:44:13|2011-07-29 12:44:13|\n",
      "|   100|39.973982524|116.303621837|      0|478.499455380577|40753.5307291667|2011-07-29|12:44:15|2011-07-29 12:44:15|\n",
      "|   100|39.973943291|116.303632641|      0|479.176988188976|40753.5307407407|2011-07-29|12:44:16|2011-07-29 12:44:16|\n",
      "|   100|39.973937148|116.303639667|      0|479.129432414698|40753.5307523148|2011-07-29|12:44:17|2011-07-29 12:44:17|\n",
      "|   100|39.973916715| 116.30363848|      0|479.615278871391|40753.5307638889|2011-07-29|12:44:18|2011-07-29 12:44:18|\n",
      "|   100|39.973892264|116.303644867|      0|480.506026902887| 40753.530775463|2011-07-29|12:44:19|2011-07-29 12:44:19|\n",
      "|   100|39.973867401|116.303647142|      0| 481.38750984252| 40753.530787037|2011-07-29|12:44:20|2011-07-29 12:44:20|\n",
      "|   100|39.973836462| 116.30365019|      0|482.008727034121|40753.5307986111|2011-07-29|12:44:21|2011-07-29 12:44:21|\n",
      "|   100|39.973821199|116.303649412|      0|482.325816929134|40753.5308101852|2011-07-29|12:44:22|2011-07-29 12:44:22|\n",
      "+------+------------+-------------+-------+----------------+----------------+----------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_concat = df.withColumn(\"DateTime\", concat_ws(\" \", df[\"Date\"], df[\"Time\"]))\n",
    "# df_concat.show()\n",
    "newDf = df_concat.withColumn(\"DateTime\", to_timestamp(\"DateTime\"))\n",
    "newDf.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aD-4_7JOV4U",
    "outputId": "c930a9ea-9a61-42b9-f970-eca114291110",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old table:\n",
      "+------+------------+-------------+----------------+----------------+----------+--------+\n",
      "|UserID|    Latitude|    Longitude|        Altitude|       Timestamp|      Date|    Time|\n",
      "+------+------------+-------------+----------------+----------------+----------+--------+\n",
      "|   100|39.974408918|116.303522101|480.287355643045|40753.5306944444|2011-07-29|12:44:12|\n",
      "|   100|39.974397078|116.303526932|480.121151574803|40753.5307060185|2011-07-29|12:44:13|\n",
      "|   100|39.973982524|116.303621837|478.499455380577|40753.5307291667|2011-07-29|12:44:15|\n",
      "|   100|39.973943291|116.303632641|479.176988188976|40753.5307407407|2011-07-29|12:44:16|\n",
      "|   100|39.973937148|116.303639667|479.129432414698|40753.5307523148|2011-07-29|12:44:17|\n",
      "|   100|39.973916715| 116.30363848|479.615278871391|40753.5307638889|2011-07-29|12:44:18|\n",
      "|   100|39.973892264|116.303644867|480.506026902887| 40753.530775463|2011-07-29|12:44:19|\n",
      "|   100|39.973867401|116.303647142| 481.38750984252| 40753.530787037|2011-07-29|12:44:20|\n",
      "|   100|39.973836462| 116.30365019|482.008727034121|40753.5307986111|2011-07-29|12:44:21|\n",
      "|   100|39.973821199|116.303649412|482.325816929134|40753.5308101852|2011-07-29|12:44:22|\n",
      "+------+------------+-------------+----------------+----------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "New table with Beijing Time:\n",
      "+------+------------+-------------+----------------+------------------+----------+--------+\n",
      "|UserID|    Latitude|    Longitude|        Altitude|         Timestamp|      Date|    Time|\n",
      "+------+------------+-------------+----------------+------------------+----------+--------+\n",
      "|   100|39.974408918|116.303522101|480.287355643045| 40753.86402777774|2011-07-29|20:44:12|\n",
      "|   100|39.974397078|116.303526932|480.121151574803|40753.864039351836|2011-07-29|20:44:13|\n",
      "|   100|39.973982524|116.303621837|478.499455380577| 40753.86406250003|2011-07-29|20:44:15|\n",
      "|   100|39.973943291|116.303632641|479.176988188976| 40753.86407407404|2011-07-29|20:44:16|\n",
      "|   100|39.973937148|116.303639667|479.129432414698|40753.864085648136|2011-07-29|20:44:17|\n",
      "|   100|39.973916715| 116.30363848|479.615278871391|40753.864097222235|2011-07-29|20:44:18|\n",
      "|   100|39.973892264|116.303644867|480.506026902887| 40753.86410879633|2011-07-29|20:44:19|\n",
      "|   100|39.973867401|116.303647142| 481.38750984252| 40753.86412037034|2011-07-29|20:44:20|\n",
      "|   100|39.973836462| 116.30365019|482.008727034121|40753.864131944436|2011-07-29|20:44:21|\n",
      "|   100|39.973821199|116.303649412|482.325816929134|40753.864143518535|2011-07-29|20:44:22|\n",
      "+------+------------+-------------+----------------+------------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF1 = newDf.withColumn(\n",
    "    \"DateTime\", newDf.DateTime + expr(\"INTERVAL +8 HOURS\")\n",
    ")  # ahead 8 hours\n",
    "# below, extrat date from 'DateTime' to 'Date', and same for 'Time'\n",
    "DF1 = (\n",
    "    DF1.withColumn(\"Date\", to_timestamp(\"DateTime\").cast(\"date\"))\n",
    "    .withColumn(\"Timestamp\", col(\"Timestamp\") + 8 / 24)\n",
    "    .withColumn(\"Time\", date_format(\"DateTime\", \"HH:mm:ss\"))\n",
    "    .drop(\"DateTime\")\n",
    ")\n",
    "\n",
    "print(\"Old table:\")\n",
    "df.drop(\"AllZero\").show(10)\n",
    "print(\"New table with Beijing Time:\")\n",
    "DF1.drop(\"AllZero\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk6yBmb85VVr"
   },
   "source": [
    "# **2.**\n",
    "\n",
    "First, group by UserID and Date, then sum each group by using count(), this will filter dupicate dates that only keep one day for each user each day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ctrRoZDQCK_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n",
      "+------+----------+-----+\n",
      "|UserID|      Date|count|\n",
      "+------+----------+-----+\n",
      "|   100|2011-07-29| 1976|\n",
      "|   100|2011-07-31|  483|\n",
      "|   100|2011-08-02|  551|\n",
      "|   100|2011-08-03|  577|\n",
      "|   100|2011-08-08|  666|\n",
      "|   100|2011-08-09| 2004|\n",
      "|   101|2007-11-30|  307|\n",
      "|   101|2007-12-02|  232|\n",
      "|   101|2007-12-03|   56|\n",
      "|   101|2007-12-07|  159|\n",
      "+------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimdf = df.select(\"UserID\", \"Date\")\n",
    "trimdf.printSchema()\n",
    "# this will count any day with at least one (>=1) data point\n",
    "question2 = trimdf.groupBy(\"UserID\", \"Date\").count().orderBy(\"UserID\", \"Date\")\n",
    "question2.show(10)\n",
    "# question2.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop useless 'count'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WcNcRbKoHUww"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|UserID|      Date|\n",
      "+------+----------+\n",
      "|   100|2011-07-29|\n",
      "|   100|2011-07-31|\n",
      "|   100|2011-08-02|\n",
      "|   100|2011-08-03|\n",
      "|   100|2011-08-08|\n",
      "|   100|2011-08-09|\n",
      "|   101|2007-11-30|\n",
      "|   101|2007-12-02|\n",
      "|   101|2007-12-03|\n",
      "|   101|2007-12-07|\n",
      "+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question2 = question2.drop(\"count\")\n",
    "question2.show(10)\n",
    "# question2.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then group by \"UserID\", sum all dates for each UserID;\n",
    "\n",
    "sort desc() will let the biggest one at first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "4Ra6ge_7Ag0m",
    "outputId": "8182da4b-d754-4448-f077-3ca1c3ab254b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|UserID|count|\n",
      "+------+-----+\n",
      "|   128|  909|\n",
      "|   126|  181|\n",
      "|   115|  123|\n",
      "|   112|  108|\n",
      "|   104|  106|\n",
      "|   125|   50|\n",
      "|   119|   44|\n",
      "|   101|   40|\n",
      "|   111|   39|\n",
      "|   102|   29|\n",
      "|   103|   22|\n",
      "|   110|   19|\n",
      "|   113|   18|\n",
      "|   114|   16|\n",
      "|   129|    9|\n",
      "|   105|    8|\n",
      "|   108|    7|\n",
      "|   100|    6|\n",
      "|   121|    5|\n",
      "|   118|    4|\n",
      "|   106|    3|\n",
      "|   120|    2|\n",
      "+------+-----+\n",
      "\n",
      "Top 5 user with number of days that has recorded:\n",
      "there are 22 units in total\n",
      "+------+-----+\n",
      "|UserID|count|\n",
      "+------+-----+\n",
      "|   128|  909|\n",
      "|   126|  181|\n",
      "|   115|  123|\n",
      "|   112|  108|\n",
      "|   104|  106|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = question2.groupBy(\"UserID\").count()  # goup by the same ID\n",
    "\n",
    "windowDept = Window.partitionBy(\"count\").orderBy(\n",
    "    col(\"count\").desc(), col(\"UserID\").asc()\n",
    ")\n",
    "# in case of a tie, output the user with the smaller ID\n",
    "df2 = (\n",
    "    df2.withColumn(\"row\", row_number().over(windowDept))\n",
    "    .filter(col(\"row\") == 1)\n",
    "    .drop(\"row\")\n",
    ")\n",
    "\n",
    "df2.orderBy(col(\"count\").desc()).show(40)\n",
    "\n",
    "\n",
    "print(\"Top 5 user with number of days that has recorded:\")\n",
    "print(\"there are %d units in total\" % df2.count())\n",
    "df2.orderBy(col(\"count\").desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_da8pgJp5ew0"
   },
   "source": [
    "# **3.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see UserID with count 31 on 2008-08-09 will be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HGRAKEZv8wNL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|UserID|      Date|count|\n",
      "+------+----------+-----+\n",
      "|   100|2011-07-29| 1976|\n",
      "|   100|2011-08-03|  577|\n",
      "|   100|2011-07-31|  483|\n",
      "|   100|2011-08-08|  666|\n",
      "|   100|2011-08-02|  551|\n",
      "|   100|2011-08-09| 2004|\n",
      "|   101|2008-04-07|  313|\n",
      "|   101|2008-05-22|  565|\n",
      "|   101|2008-05-09|   31|\n",
      "|   101|2008-03-24|  123|\n",
      "+------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+-----+\n",
      "|UserID|      Date|count|\n",
      "+------+----------+-----+\n",
      "|   100|2011-08-02|  551|\n",
      "|   100|2011-08-03|  577|\n",
      "|   100|2011-07-31|  483|\n",
      "|   100|2011-08-08|  666|\n",
      "|   100|2011-07-29| 1976|\n",
      "|   100|2011-08-09| 2004|\n",
      "|   101|2008-06-29| 2469|\n",
      "|   101|2007-12-07|  159|\n",
      "|   101|2008-02-02|  612|\n",
      "|   101|2007-12-15|  112|\n",
      "+------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first remove the dublicated dates\n",
    "newdf = trimdf.groupBy(\"UserID\", \"Date\").count().orderBy(\"UserID\")\n",
    "newdf.show(10)\n",
    "# filter any day which more than 100 data points recorded\n",
    "question3 = newdf.filter(\"count >=100\")\n",
    "question3.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXQY9yFSMCfh",
    "outputId": "1060b70d-b030-489a-a6d3-c02aea4f6ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all user IDs that with number of days that at least 100 data points per day:\n",
      "+------+--------------+\n",
      "|UserID|number of days|\n",
      "+------+--------------+\n",
      "|   128|           875|\n",
      "|   126|           175|\n",
      "|   115|           110|\n",
      "|   112|            98|\n",
      "|   104|            77|\n",
      "|   125|            47|\n",
      "|   119|            41|\n",
      "|   101|            31|\n",
      "|   102|            26|\n",
      "|   103|            22|\n",
      "|   111|            21|\n",
      "|   130|            19|\n",
      "|   122|            18|\n",
      "|   113|            18|\n",
      "|   114|            15|\n",
      "|   110|            13|\n",
      "|   105|             8|\n",
      "|   124|             7|\n",
      "|   127|             7|\n",
      "|   100|             6|\n",
      "|   121|             5|\n",
      "|   123|             4|\n",
      "|   106|             3|\n",
      "|   116|             3|\n",
      "|   129|             3|\n",
      "|   118|             3|\n",
      "|   108|             2|\n",
      "|   120|             2|\n",
      "|   109|             2|\n",
      "+------+--------------+\n",
      "\n",
      "there are 29 units in total\n"
     ]
    }
   ],
   "source": [
    "question3 = question3.groupBy(\"UserID\").count()  # goup and by the same ID\n",
    "question3 = question3.withColumnRenamed(\"count\", \"number of days\")\n",
    "size = question3.count()\n",
    "\n",
    "print(\"all user IDs that with number of days that at least 100 data points per day:\")\n",
    "question3.sort(col(\"number of days\").desc()).show(size)\n",
    "print(\"there are %d units in total\" % size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuXXMxFsKx5P"
   },
   "source": [
    "# **4.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first select useful colum, and make sure the data types are correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ59ekWXa4Em",
    "outputId": "baed7ff1-24bd-4104-b0f2-bf76c9bf214b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------+\n",
      "|UserID|        Altitude|      Date|\n",
      "+------+----------------+----------+\n",
      "|   100|480.287355643045|2011-07-29|\n",
      "|   100|480.121151574803|2011-07-29|\n",
      "|   100|478.499455380577|2011-07-29|\n",
      "|   100|479.176988188976|2011-07-29|\n",
      "|   100|479.129432414698|2011-07-29|\n",
      "|   100|479.615278871391|2011-07-29|\n",
      "|   100|480.506026902887|2011-07-29|\n",
      "|   100| 481.38750984252|2011-07-29|\n",
      "|   100|482.008727034121|2011-07-29|\n",
      "|   100|482.325816929134|2011-07-29|\n",
      "|   100|482.289422572178|2011-07-29|\n",
      "|   100|482.314353674541|2011-07-29|\n",
      "|   100|482.362713254593|2011-07-29|\n",
      "|   100|482.472345800525|2011-07-29|\n",
      "|   100|482.716797900262|2011-07-29|\n",
      "|   100|482.782529527559|2011-07-29|\n",
      "|   100|483.086404199475|2011-07-29|\n",
      "|   100|483.396486220472|2011-07-29|\n",
      "|   100|483.624166666667|2011-07-29|\n",
      "|   100|484.271414041995|2011-07-29|\n",
      "+------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- Altitude: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "altitudedf = df.select(\"UserID\", \"Altitude\", \"Date\")\n",
    "altitudedf.show()\n",
    "altitudedf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first, Partition the DataFrame on \"UserID\" column, which groups all same departments into a group.\n",
    "\n",
    "- Apply orderBy() on \"Altitude\" column by descending order, \"Date\" for ascending to make sure output the earliest day if tie.\n",
    "- Add a new column row by running row_number() function over the partition window. row_number() function returns a sequential number starting from 1 within a window partition group.\n",
    "- Using the PySpark filter(), just select row == 1, which returns the maximum \"Altitude\" of each group.\n",
    "- Finally, if a row column is not needed, just drop it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmJ9b_FpmWiG",
    "outputId": "89cebb3d-1c4e-451d-feec-76820d2b3401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 user with the highest altitude:\n",
      "+------+----------------+----------+\n",
      "|UserID|        Altitude|      Date|\n",
      "+------+----------------+----------+\n",
      "|   128|        107503.3|2009-11-02|\n",
      "|   106|36581.3648293963|2007-10-09|\n",
      "|   103|         25259.2|2008-09-12|\n",
      "|   101|         24806.4|2008-03-28|\n",
      "|   126|         19432.4|2008-06-22|\n",
      "+------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-find-maximum-row-per-group-in-dataframe/\n",
    "# col(\"Date\").asc() means in case of a tie, output the earliest such a day\n",
    "windowDept = Window.partitionBy(\"UserID\").orderBy(\n",
    "    col(\"Altitude\").desc(), col(\"Date\").asc()\n",
    ")\n",
    "question4 = (\n",
    "    altitudedf.withColumn(\"row\", row_number().over(windowDept))\n",
    "    .filter(col(\"row\") == 1)\n",
    "    .drop(\"row\")\n",
    ")\n",
    "print(\"Top 5 user with the highest altitude:\")\n",
    "question4.orderBy(col(\"Altitude\").desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC-EAPAMQZuE"
   },
   "source": [
    "# **5.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "4jEaiNRmQ8VW"
   },
   "outputs": [],
   "source": [
    "trimdf = df.select(\"UserID\", \"Timestamp\")\n",
    "# trimdf.show()\n",
    "# make sure the data type are correct\n",
    "question5 = trimdf.withColumn(\"Timestamp\", trimdf.Timestamp.cast(DoubleType()))\n",
    "# question5.printSchema()\n",
    "\n",
    "# qaq=question5.groupBy('UserID','Timestamp').count()\n",
    "question5 = question5.sort(col(\"UserID\").asc())\n",
    "# question5.show()\n",
    "# question5.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same principle as question4, this time also include minimum, the only difference is order byTimestamp ascend or descendly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7rsIffHX-OY",
    "outputId": "82e3b9a9-d094-45bf-d40c-bc23402f340d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID with Minimum TimeStamp list:\n",
      "+------+----------------+\n",
      "|UserID|   Min_Timestamp|\n",
      "+------+----------------+\n",
      "|   100|40753.5306944444|\n",
      "|   101|39416.6085300926|\n",
      "|   102|40833.5650810185|\n",
      "|   103|39687.2470023148|\n",
      "|   104|39416.6075347222|\n",
      "|   105|39357.0945833333|\n",
      "|   106|39363.0810763889|\n",
      "|   107|39357.3149189815|\n",
      "|   108|39357.6008680556|\n",
      "|   109|39416.6085300926|\n",
      "+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "UserID with Maximum TimeStamp list:\n",
      "+------+----------------+\n",
      "|UserID|   Max_Timestamp|\n",
      "+------+----------------+\n",
      "|   100|40764.6604861111|\n",
      "|   101|39629.3044791667|\n",
      "|   102|40908.6375810185|\n",
      "|   103|39711.4569097222|\n",
      "|   104|39733.8255092593|\n",
      "|   105|39365.4873842593|\n",
      "|   106|39365.5583449074|\n",
      "|   107|39362.0612962963|\n",
      "|   108|39364.0281481481|\n",
      "|   109|39419.6934027778|\n",
      "+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowDept = Window.partitionBy(\"UserID\").orderBy(col(\"Timestamp\").asc())\n",
    "min = (\n",
    "    question5.withColumn(\"row\", row_number().over(windowDept))\n",
    "    .filter(col(\"row\") == 1)\n",
    "    .drop(\"row\")\n",
    "    .withColumnRenamed(\"Timestamp\", \"Min_Timestamp\")\n",
    ")\n",
    "print(\"UserID with Minimum TimeStamp list:\")\n",
    "min.show(10)\n",
    "\n",
    "windowDept = Window.partitionBy(\"UserID\").orderBy(col(\"Timestamp\").desc())\n",
    "max = (\n",
    "    question5.withColumn(\"row\", row_number().over(windowDept))\n",
    "    .filter(col(\"row\") == 1)\n",
    "    .drop(\"row\")\n",
    "    .withColumnRenamed(\"Timestamp\", \"Max_Timestamp\")\n",
    ")\n",
    "print(\"UserID with Maximum TimeStamp list:\")\n",
    "max.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join the minimum and maximum timestamp to one dataframe and prepare to be calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "FXV0unqmhzEg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|UserID|          Timespan|\n",
      "+------+------------------+\n",
      "|   100|11.129791666695382|\n",
      "|   101| 212.6959490740992|\n",
      "|   102| 75.07249999999476|\n",
      "|   103| 24.20990740739944|\n",
      "|   104|317.21797453710315|\n",
      "|   105| 8.392800925998017|\n",
      "|   106| 2.477268518494384|\n",
      "|   107| 4.746377314797428|\n",
      "|   108|6.4272800925027695|\n",
      "|   109|3.0848726851982065|\n",
      "+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question5 = max.join(min, max.UserID == min.UserID).drop(min.UserID)\n",
    "# caclulate the time span for each user\n",
    "question5 = question5.withColumn(\n",
    "    \"Timespan\", (question5[\"Max_Timestamp\"] - question5[\"Min_Timestamp\"])\n",
    ")\n",
    "question5 = question5.drop(\"Max_Timestamp\", \"Min_Timestamp\")\n",
    "question5.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPYarbwqlDOX",
    "outputId": "afba3e05-ff9d-49fd-cb7c-ee4d6e0ad79e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 highest Timespan UserID:\n",
      "+------+-----------------+\n",
      "|UserID|         Timespan|\n",
      "+------+-----------------+\n",
      "|   128|1426.294375000005|\n",
      "|   114|963.8455902778005|\n",
      "|   111|838.7832175925942|\n",
      "|   115|506.6880439814995|\n",
      "|   126| 325.831643518497|\n",
      "+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 highest Timespan UserID:\")\n",
    "question5.sort(col(\"Timespan\").desc()).show(5)\n",
    "# print(\"there are %d units in total\" % question5.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly, create a new frame that only contain neccessary colums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+----------+----------------+\n",
      "|UserID|    Latitude|    Longitude|      Date|       Timestamp|\n",
      "+------+------------+-------------+----------+----------------+\n",
      "|   100|39.974408918|116.303522101|2011-07-29|40753.5306944444|\n",
      "|   100|39.974397078|116.303526932|2011-07-29|40753.5307060185|\n",
      "|   100|39.973982524|116.303621837|2011-07-29|40753.5307291667|\n",
      "|   100|39.973943291|116.303632641|2011-07-29|40753.5307407407|\n",
      "|   100|39.973937148|116.303639667|2011-07-29|40753.5307523148|\n",
      "|   100|39.973916715| 116.30363848|2011-07-29|40753.5307638889|\n",
      "|   100|39.973892264|116.303644867|2011-07-29| 40753.530775463|\n",
      "|   100|39.973867401|116.303647142|2011-07-29| 40753.530787037|\n",
      "|   100|39.973836462| 116.30365019|2011-07-29|40753.5307986111|\n",
      "|   100|39.973821199|116.303649412|2011-07-29|40753.5308101852|\n",
      "+------+------------+-------------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimdf = df.select(\"UserID\", \"Latitude\", \"Longitude\", \"Date\", \"Timestamp\").orderBy(\n",
    "    col(\"UserID\").asc(), col(\"Timestamp\").asc()\n",
    ")\n",
    "trimdf.show(10)\n",
    "# trimdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a window below, and then lag each Latitude and Longitude one row, partition by UserID, so each UserID's first row shoud be null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+----------+----------------+-------------+--------------+\n",
      "|UserID|    Latitude|    Longitude|      Date|       Timestamp|prev_Latitude|prev_Longitude|\n",
      "+------+------------+-------------+----------+----------------+-------------+--------------+\n",
      "|   100|39.974408918|116.303522101|2011-07-29|40753.5306944444|         null|          null|\n",
      "|   100|39.974397078|116.303526932|2011-07-29|40753.5307060185| 39.974408918| 116.303522101|\n",
      "|   100|39.973982524|116.303621837|2011-07-29|40753.5307291667| 39.974397078| 116.303526932|\n",
      "|   100|39.973943291|116.303632641|2011-07-29|40753.5307407407| 39.973982524| 116.303621837|\n",
      "|   100|39.973937148|116.303639667|2011-07-29|40753.5307523148| 39.973943291| 116.303632641|\n",
      "|   100|39.973916715| 116.30363848|2011-07-29|40753.5307638889| 39.973937148| 116.303639667|\n",
      "|   100|39.973892264|116.303644867|2011-07-29| 40753.530775463| 39.973916715|  116.30363848|\n",
      "|   100|39.973867401|116.303647142|2011-07-29| 40753.530787037| 39.973892264| 116.303644867|\n",
      "|   100|39.973836462| 116.30365019|2011-07-29|40753.5307986111| 39.973867401| 116.303647142|\n",
      "|   100|39.973821199|116.303649412|2011-07-29|40753.5308101852| 39.973836462|  116.30365019|\n",
      "+------+------------+-------------+----------+----------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"UserID\").orderBy(col(\"Timestamp\").asc())\n",
    "df_lag = trimdf.withColumn(\n",
    "    \"prev_Latitude\", lag(trimdf[\"Latitude\"]).over(windowSpec)\n",
    ").withColumn(\"prev_Longitude\", lag(trimdf[\"Longitude\"]).over(windowSpec))\n",
    "# df_lag=df_lag.na.fill(value=0,subset=[\"prev_Latitude\",\"prev_Longitude\"])\n",
    "df_lag.filter(col(\"UserID\") == 100).show(10)\n",
    "# df_lag.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To express sin, cos and radian function, I will need sql function form pyspark,\n",
    "\n",
    "the caculation method is from: https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+----------+------------------+\n",
      "|UserID|    Latitude|    Longitude|      Date|   distance(meter)|\n",
      "+------+------------+-------------+----------+------------------+\n",
      "|   100|39.974408918|116.303522101|2011-07-29|              null|\n",
      "|   100|39.974397078|116.303526932|2011-07-29|1.3798395802400303|\n",
      "|   100|39.973982524|116.303621837|2011-07-29| 46.81501218227461|\n",
      "|   100|39.973943291|116.303632641|2011-07-29| 4.459995088445842|\n",
      "|   100|39.973937148|116.303639667|2011-07-29|0.9085982359806887|\n",
      "|   100|39.973916715| 116.30363848|2011-07-29|2.2750102315565335|\n",
      "|   100|39.973892264|116.303644867|2011-07-29|2.7736368609033817|\n",
      "|   100|39.973867401|116.303647142|2011-07-29| 2.772297938081956|\n",
      "|   100|39.973836462| 116.30365019|2011-07-29|  3.45113330739803|\n",
      "|   100|39.973821199|116.303649412|2011-07-29|1.6989956937999133|\n",
      "+------+------------+-------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "distance_list = (\n",
    "    df_lag.withColumn(\n",
    "        \"a\",\n",
    "        F.sin((F.radians(col(\"Latitude\")) - F.radians(col(\"prev_Latitude\"))) / 2) ** 2\n",
    "        + F.cos((F.radians(col(\"prev_Latitude\"))))\n",
    "        * F.cos((F.radians(col(\"Latitude\"))))\n",
    "        * F.sin((F.radians(col(\"Longitude\")) - F.radians(col(\"prev_Longitude\"))) / 2)\n",
    "        ** 2,\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"distance(meter)\",\n",
    "        2 * F.atan2(F.sqrt(F.col(\"a\")), F.sqrt(1 - F.col(\"a\"))) * 6373.0 * 1000,\n",
    "    )\n",
    "    .drop(\"a\")\n",
    ")\n",
    "# show some of rows to check\n",
    "distance_list.filter(col(\"UserID\") == 100).drop(\n",
    "    \"TimeStamp\", \"prev_Latitude\", \"prev_Longitude\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking computations for the first value where UserID = 100:\n",
    "As we can see the first row from distance_list is almost the same as mpu caculated, the are both around 1.379 meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3794065535071975"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install mpu\n",
    "import mpu\n",
    "\n",
    "lat1 = 39.974408918\n",
    "lon1 = 116.303522101\n",
    "lat2 = 39.974397078\n",
    "lon2 = 116.303526932\n",
    "\n",
    "mpu.haversine_distance((lat1, lon1), (lat2, lon2)) * 1000  # km->m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by UserID and sum each group's distance(meter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|UserID|      Date|sum(distance(meter))|\n",
      "+------+----------+--------------------+\n",
      "|   100|2011-07-29|   10968.19726137189|\n",
      "|   100|2011-08-09|   9859.201742002968|\n",
      "|   100|2011-08-03|  3007.3359004551553|\n",
      "|   100|2011-08-08|  2992.7149981476996|\n",
      "|   100|2011-07-31|  2914.2650684240202|\n",
      "|   100|2011-08-02|  2322.4813446678218|\n",
      "|   101|2008-01-25|   912636.5609156062|\n",
      "|   101|2008-03-02|   910255.5372850713|\n",
      "|   101|2007-12-22|  222879.25910870914|\n",
      "|   101|2008-02-02|  189791.11993210213|\n",
      "|   101|2008-03-09|   185714.6327218315|\n",
      "|   101|2007-12-19|  157989.98987679632|\n",
      "|   101|2007-12-15|  134268.30041232018|\n",
      "|   101|2007-12-13|   131311.6597017672|\n",
      "|   101|2008-03-12|  114941.11641117312|\n",
      "|   101|2008-03-30|   97868.81779553965|\n",
      "|   101|2008-03-28|    90380.8490922154|\n",
      "|   101|2007-12-08|   73516.50969499162|\n",
      "|   101|2008-06-29|   39708.42500351011|\n",
      "|   101|2007-11-30|   35724.77945028216|\n",
      "+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question6 = (\n",
    "    distance_list.groupBy(\"UserID\", \"Date\")\n",
    "    .sum(\"distance(meter)\")\n",
    "    .drop(\"TimeStamp\", \"prev_Latitude\", \"prev_Longitude\")\n",
    ").orderBy(\"UserID\", col(\"sum(distance(meter))\").desc())\n",
    "question6.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user output the (earliest) day they travelled the most, So sort by both Date(asc for earliest) and distance(desc for largest), same method using window as question 4 and 5,\n",
    "\n",
    "as we can see the largest one is User 128, who is also the longest collecting time user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (earliest) day each User travelled the most: \n",
      "+------+----------+--------------------+\n",
      "|UserID|      Date| Max_Distance(meter)|\n",
      "+------+----------+--------------------+\n",
      "|   128|2009-02-22|1.0093183885299848E7|\n",
      "|   124|2008-10-03|   4277820.620757491|\n",
      "|   111|2007-09-05|   2462793.930192698|\n",
      "|   115|2007-11-28|  2098104.3645122917|\n",
      "|   122|2009-07-21|   2005624.137404955|\n",
      "|   125|2008-08-27|  1597832.1468177768|\n",
      "|   112|2008-02-02|  1078721.9733951034|\n",
      "|   127|2008-10-05|  1028823.4643552401|\n",
      "|   123|2009-09-23|   964125.4511769849|\n",
      "|   101|2008-01-25|   912636.5609156062|\n",
      "|   120|2009-09-19|  464563.23009168444|\n",
      "|   126|2008-05-01|  372628.64234369097|\n",
      "|   129|2008-05-02|   317812.7642064925|\n",
      "|   106|2007-10-08|  269624.04448305944|\n",
      "|   118|2007-05-20|  223659.21346841284|\n",
      "|   103|2008-09-12|  176983.89397442676|\n",
      "|   110|2008-01-18|   159683.2755984129|\n",
      "|   119|2008-09-02|  156787.47962323824|\n",
      "|   108|2007-10-04|  147051.66030393587|\n",
      "|   121|2009-10-08|  135596.80158893982|\n",
      "|   104|2007-12-18|  105553.93745442806|\n",
      "|   130|2009-07-12|  103374.00524879836|\n",
      "|   105|2007-10-05|   79004.70622294117|\n",
      "|   113|2010-05-31|   65424.03538942799|\n",
      "|   114|2010-05-28|  46584.421404454704|\n",
      "|   109|2007-11-30|   35724.77945028216|\n",
      "|   102|2011-12-31|  31248.569886198304|\n",
      "|   117|2007-06-22|  26317.268727348568|\n",
      "|   100|2011-07-29|   10968.19726137189|\n",
      "|   107|2007-10-07|   8662.449774309242|\n",
      "|   116|2011-08-03|   5621.078133668588|\n",
      "+------+----------+--------------------+\n",
      "\n",
      "there are 31 units in total\n"
     ]
    }
   ],
   "source": [
    "windowDept = Window.partitionBy(\"UserID\").orderBy(\n",
    "    col(\"sum(distance(meter))\").desc(), col(\"Date\").asc()\n",
    ")  # make sure it is also earliest day in each first row\n",
    "max = (\n",
    "    question6.withColumn(\"row\", row_number().over(windowDept))\n",
    "    .filter(col(\"row\") == 1)\n",
    "    .drop(\"row\")\n",
    "    .withColumnRenamed(\"sum(distance(meter))\", \"Max_Distance(meter)\")\n",
    ")\n",
    "\n",
    "size = max.count()\n",
    "print(\"The (earliest) day each User travelled the most: \")\n",
    "max.orderBy(col(\"Max_Distance(meter)\").desc()).show(size)\n",
    "print(\"there are %d units in total\" % size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just simply sum all sumed users to get the total distance, divided by 1000 to turn metric to kilometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Total Distance(km)|\n",
      "+------------------+\n",
      "| 180859.0390624603|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question6.select(sum(\"sum(distance(meter))\")).withColumn(\n",
    "    \"Total Distance(km)\", col(\"sum(sum(distance(meter)))\") / 1000\n",
    ").drop(\"sum(sum(distance(meter)))\").show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dsmJM3Y-5Lu0",
    "xk6yBmb85VVr",
    "_da8pgJp5ew0",
    "AuXXMxFsKx5P",
    "QC-EAPAMQZuE"
   ],
   "name": "COMP529 Assignment 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
